{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7tlFDutHXse",
        "outputId": "19332794-7857-4e2a-f0e0-5bf3428491cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 15 10:23:55 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0              46W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4ovHO8LDTWA",
        "outputId": "616c1c0a-a8aa-4394-a2c2-917a362eb23d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.7/777.7 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install lightning --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKndHUAhDZo2",
        "outputId": "1a92d0b0-ee99-4593-a27c-1a37494ee0d8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/capstone')\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXeOp9R2DaWo",
        "outputId": "32c3f283-7987-41a9-8e55-1142decf962c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " capstone_phi2_pretraining.ipynb   data\t\t       out\t\t\t tsai_gpt\n",
            " capstone.zip\t\t\t   dataset.py\t       pretraining_phi_2.ipynb\n",
            " checkpoints\t\t\t  'import socket.py'   results\n",
            " client.ipynb\t\t\t   main.ipynb\t       server.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-W4fhUUDTWB",
        "outputId": "7c21baa6-aeec-4103-f2e6-8fc3f7cb7e72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n",
            "2.1.3\n",
            "3.10.12\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import lightning\n",
        "print(torch.__version__)\n",
        "print(lightning.__version__)\n",
        "from platform import python_version\n",
        "print(python_version())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfoZRxRvombq",
        "outputId": "08d1fc7c-f6ef-44c0-c776-ddee4a17278d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SmkaHwPI6k8s"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import bitsandbytes as bnb\n",
        "import gc\n",
        "import glob\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Union\n",
        "import lightning as L\n",
        "import torch.nn as nn\n",
        "from lightning.fabric.loggers import CSVLogger\n",
        "from lightning.fabric.strategies import FSDPStrategy\n",
        "from torch.utils.data import DataLoader\n",
        "from tsai_gpt.model import GPT, Block, Config\n",
        "from tsai_gpt.packed_dataset import CombinedDataset, PackedDataset\n",
        "from tsai_gpt.speed_monitor import SpeedMonitorBase, estimate_flops, measure_flops\n",
        "from tsai_gpt.speed_monitor import SpeedMonitorFabric as SpeedMonitor\n",
        "from tsai_gpt.utils import chunked_cross_entropy, get_default_supported_precision, num_parameters, load_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FSJ4kDcn6k8t"
      },
      "outputs": [],
      "source": [
        "# Configuration for running the model\n",
        "model_name = \"phi-1_5\"\n",
        "name = \"phi-1_5\"\n",
        "out_dir = Path(\"out\") / name\n",
        "save_interval = 1000\n",
        "eval_interval = 1000\n",
        "eval_iters = 100\n",
        "log_interval = 100\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 6e-3\n",
        "batch_size = 1\n",
        "micro_batch_size = 1\n",
        "gradient_accumulation_steps = 4 #batch_size // micro_batch_size\n",
        "assert gradient_accumulation_steps > 0\n",
        "#max_iters = 600000  # num_epochs * (epoch_size // micro_batch_size) // devices\n",
        "max_iters = 10000\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "decay_lr = True\n",
        "warmup_iters = 2000\n",
        "lr_decay_iters = max_iters\n",
        "min_lr = 6e-6\n",
        "\n",
        "# Data configurations\n",
        "data_config = [\n",
        "    # (\"arxiv\", 2.5),\n",
        "    # (\"book\", 4.5),\n",
        "    (\"c4\", 20.0),\n",
        "    (\"cc\", 80.0),\n",
        "    # (\"github\", 4.5),\n",
        "    # (\"stackexchange\", 2.0),\n",
        "    # (\"wikipedia\", 4.5),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Data from zip file"
      ],
      "metadata": {
        "id": "oMbwqm6DRwPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "with ZipFile('data/sample100MB.zip', 'r') as f:\n",
        "\n",
        "#extract in designated directory\n",
        "  f.extractall('data')"
      ],
      "metadata": {
        "id": "zflE41E0c_wQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "P2vK2-xO6k8t"
      },
      "outputs": [],
      "source": [
        "## setup - main function\n",
        "hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}\n",
        "logger = CSVLogger(\"out\", name, flush_logs_every_n_steps=log_interval)\n",
        "\n",
        "\n",
        "def setup(\n",
        "    devices: int = 4,\n",
        "    train_data_dir: Path = Path(\"data/capstone\"),\n",
        "    val_data_dir: Optional[Path] = None,\n",
        "    precision: Optional[str] = None,\n",
        "    resume: Union[bool, Path] = False,\n",
        ") -> None:\n",
        "    precision = precision or get_default_supported_precision(training=True)\n",
        "\n",
        "    if devices > 1:\n",
        "        strategy = FSDPStrategy(\n",
        "            auto_wrap_policy={Block},\n",
        "            activation_checkpointing_policy={Block},\n",
        "            state_dict_type=\"full\",\n",
        "            limit_all_gathers=True,\n",
        "            cpu_offload=False,\n",
        "        )\n",
        "    else:\n",
        "        strategy = \"auto\"\n",
        "\n",
        "    fabric = L.Fabric(devices=devices, strategy=strategy, precision=precision, loggers=logger)\n",
        "    fabric.print(hparams)\n",
        "    fabric.launch(main, train_data_dir, val_data_dir, resume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vNtZq5gP6k8u"
      },
      "outputs": [],
      "source": [
        "## setup launches main function\n",
        "def main(fabric: L.Fabric, train_data_dir: Path, val_data_dir: Path, resume: Union[bool, Path]) -> None:\n",
        "    global model_copy\n",
        "    speed_monitor = SpeedMonitor(fabric, window_size=50, time_unit=\"seconds\")\n",
        "\n",
        "    if fabric.global_rank == 0:\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    config = Config.from_name(model_name)\n",
        "\n",
        "    train_dataloader, val_dataloader = create_dataloaders(\n",
        "        batch_size=micro_batch_size,\n",
        "        block_size=config.block_size,\n",
        "        fabric=fabric,\n",
        "        train_data_dir=train_data_dir,\n",
        "        val_data_dir=val_data_dir,\n",
        "        seed = (1337 + fabric.global_rank),\n",
        "    )\n",
        "    if val_dataloader is None:\n",
        "        train_dataloader = fabric.setup_dataloaders(train_dataloader)\n",
        "    else:\n",
        "        train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)\n",
        "\n",
        "    fabric.seed_everything(1337)  # same seed for every process to init model (FSDP)\n",
        "    fabric.print(f\"Loading model with {config.__dict__}\")\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    def _init_weights(module: nn.Module) -> None:\n",
        "            \"\"\"Meant to be used with `gpt.apply(gpt._init_weights)`.\"\"\"\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    with fabric.init_module(empty_init=True):\n",
        "        model = GPT(config)\n",
        "        model.apply(_init_weights)\n",
        "    model.apply(_init_weights)\n",
        "\n",
        "    fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\")\n",
        "    fabric.print(f\"Total parameters {num_parameters(model):,}\")\n",
        "\n",
        "    model = fabric.setup(model)\n",
        "    optimizer = bnb.optim.Adam8bit(\n",
        "        model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2) #, foreach=False\n",
        "    )\n",
        "    optimizer = fabric.setup_optimizers(optimizer)\n",
        "    state = {\"model\": model, \"optimizer\": optimizer, \"hparams\": hparams, \"iter_num\": 0, \"step_count\": 0}\n",
        "\n",
        "    if resume is True:\n",
        "        resume = max(out_dir.glob(\"*.pth\"), key=lambda p: int(p.name.split(\"-\")[1]))\n",
        "    if resume:\n",
        "        fabric.print(f\"Resuming training from {resume}\")\n",
        "        fabric.load(resume, state)\n",
        "\n",
        "    train_time = time.perf_counter()\n",
        "    train(fabric, state, train_dataloader, val_dataloader, speed_monitor)\n",
        "    fabric.print(f\"Training time: {(time.perf_counter()-train_time):.2f}s\")\n",
        "    if fabric.device.type == \"cuda\":\n",
        "        fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kMl2k2D76k8u"
      },
      "outputs": [],
      "source": [
        "## train\n",
        "def train(\n",
        "    fabric: L.Fabric,\n",
        "    state: dict,\n",
        "    train_dataloader: DataLoader,\n",
        "    val_dataloader: DataLoader,\n",
        "    speed_monitor: SpeedMonitorBase,\n",
        ") -> None:\n",
        "    model     = state[\"model\"]\n",
        "    optimizer = state[\"optimizer\"]\n",
        "\n",
        "    if val_dataloader is not None:\n",
        "        validate(fabric, model, val_dataloader)  # sanity check\n",
        "\n",
        "    with torch.device(\"meta\"):\n",
        "        meta_model = GPT(model.config)\n",
        "        # \"estimated\" is not as precise as \"measured\". Estimated is optimistic but widely used in the wild.\n",
        "        # When comparing MFU or FLOP numbers with other projects that use estimated FLOPs,\n",
        "        # consider passing `SpeedMonitor(flops_per_batch=estimated_flops)` instead\n",
        "        estimated_flops = estimate_flops(meta_model) * micro_batch_size\n",
        "        fabric.print(f\"Estimated TFLOPs: {estimated_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        x = torch.randint(0, 1, (micro_batch_size, model.max_seq_length))\n",
        "        measured_flops = measure_flops(meta_model, x)\n",
        "        fabric.print(f\"Measured TFLOPs: {measured_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        del meta_model, x\n",
        "\n",
        "    total_lengths = 0\n",
        "    total_t0 = time.perf_counter()\n",
        "\n",
        "    for state[\"iter_num\"], train_data in enumerate(train_dataloader, state[\"iter_num\"]):\n",
        "        if state[\"iter_num\"] >= max_iters:\n",
        "            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
        "            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n",
        "            fabric.save(checkpoint_path, state)\n",
        "            break\n",
        "\n",
        "        # determine and set the learning rate for this iteration\n",
        "        lr = get_lr(state[\"iter_num\"]) if decay_lr else learning_rate\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "        iter_t0 = time.perf_counter()\n",
        "\n",
        "        input_ids = train_data[:, 0 : model.max_seq_length].contiguous()\n",
        "        targets = train_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
        "\n",
        "        is_accumulating = (state[\"iter_num\"] + 1) % gradient_accumulation_steps != 0\n",
        "        with fabric.no_backward_sync(model, enabled=is_accumulating):\n",
        "            logits = model(input_ids)\n",
        "            loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "            fabric.backward(loss / gradient_accumulation_steps)\n",
        "\n",
        "        if not is_accumulating:\n",
        "            fabric.clip_gradients(model, optimizer, max_norm=grad_clip)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            state[\"step_count\"] += 1\n",
        "\n",
        "        t1 = time.perf_counter()\n",
        "        total_lengths += input_ids.size(1)\n",
        "        speed_monitor.on_train_batch_end(\n",
        "            (state[\"iter_num\"] + 1) * micro_batch_size,\n",
        "            t1 - total_t0,\n",
        "            # this assumes that device FLOPs are the same and that all devices have the same batch size\n",
        "            fabric.world_size,\n",
        "            flops_per_batch=measured_flops,\n",
        "            lengths=total_lengths,\n",
        "        )\n",
        "        if state[\"iter_num\"] % log_interval == 0:\n",
        "            fabric.print(\n",
        "                f\"iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}, LR: {lr:.6f}, iter time:\"\n",
        "                f\" {(t1 - iter_t0) * 1000:.2f}ms{' (optimizer.step)' if not is_accumulating else ''}\"\n",
        "            )\n",
        "        if val_dataloader is not None and not is_accumulating and state[\"step_count\"] % eval_interval == 0:\n",
        "            t0 = time.perf_counter()\n",
        "            val_loss = validate(fabric, model, val_dataloader)\n",
        "            t1 = time.perf_counter() - t0\n",
        "            speed_monitor.eval_end(t1)\n",
        "            fabric.print(f\"step {state['iter_num']}: val loss {val_loss.item():.4f}, val time: {t1 * 1000:.2f}ms\")\n",
        "            fabric.barrier()\n",
        "        if not is_accumulating and state[\"step_count\"] % save_interval == 0:\n",
        "            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
        "            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n",
        "            fabric.save(checkpoint_path, state)\n",
        "\n",
        "\n",
        "## inference\n",
        "@torch.inference_mode()\n",
        "def validate(fabric: L.Fabric, model: torch.nn.Module, val_dataloader: DataLoader) -> torch.Tensor:\n",
        "    fabric.print(\"Validating ...\")\n",
        "    model.eval()\n",
        "\n",
        "    losses = torch.zeros(eval_iters, device=fabric.device)\n",
        "    for k, val_data in enumerate(val_dataloader):\n",
        "        input_ids = val_data[:, 0 : model.max_seq_length].contiguous()\n",
        "        targets = val_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
        "        logits = model(input_ids)\n",
        "        losses[k] = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "    out = losses.mean()\n",
        "\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "G7hFTpFk6k8v"
      },
      "outputs": [],
      "source": [
        "## dataloaders\n",
        "def create_dataloader(\n",
        "    batch_size: int, block_size: int, data_dir: Path, fabric: L.Fabric, shuffle: bool = True, seed: int = 12345\n",
        ") -> DataLoader:\n",
        "    datasets = []\n",
        "    for prefix, _ in data_config:\n",
        "        filenames = glob.glob(str(data_dir / f\"{prefix}*\"))\n",
        "        dataset = PackedDataset(\n",
        "            filenames,\n",
        "            n_chunks=1,\n",
        "            block_size=block_size,\n",
        "            shuffle=shuffle,\n",
        "            seed=seed,\n",
        "            num_processes=fabric.world_size,\n",
        "            process_rank=fabric.global_rank,\n",
        "        )\n",
        "        datasets.append(dataset)\n",
        "    if not datasets:\n",
        "        raise RuntimeError(\n",
        "            f\"No data found at {data_dir}. Make sure you ran prepare_redpajama.py to create the dataset.\"\n",
        "        )\n",
        "    weights = [weight for _, weight in data_config]\n",
        "    sum_weights = sum(weights)\n",
        "    weights = [el / sum_weights for el in weights]\n",
        "    combined_dataset = CombinedDataset(datasets=datasets, seed=seed, weights=weights)\n",
        "\n",
        "    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
        "\n",
        "def create_dataloaders(\n",
        "    batch_size: int,\n",
        "    block_size: int,\n",
        "    fabric: L.Fabric,\n",
        "    train_data_dir: Path = Path(\"data/sample\"),\n",
        "    val_data_dir: Optional[Path] = None,\n",
        "    seed: int = 12345,\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "    # Increase by one because we need the next word as well\n",
        "    effective_block_size = block_size + 1\n",
        "    train_dataloader = create_dataloader(\n",
        "        batch_size=batch_size,\n",
        "        block_size=effective_block_size,\n",
        "        fabric=fabric,\n",
        "        data_dir=train_data_dir,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "    )\n",
        "    val_dataloader = (\n",
        "        create_dataloader(\n",
        "            batch_size=batch_size,\n",
        "            block_size=effective_block_size,\n",
        "            fabric=fabric,\n",
        "            data_dir=val_data_dir,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "        )\n",
        "        if val_data_dir\n",
        "        else None\n",
        "    )\n",
        "    return train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "keSWtdpO6k8v"
      },
      "outputs": [],
      "source": [
        "## Learning rate scheduler\n",
        "def get_lr(it: int) -> float:\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "rHdKLbfcHKw7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieVPQlV-o96S",
        "outputId": "e830f974-51b3-4763-e974-35ff0c74ba41"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "z_4BAV0O6k8v",
        "outputId": "42659936-b71a-4324-a685-e30bd364702e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO: You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
            "INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-40GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model_name': 'phi-1_5', 'name': 'phi-1_5', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 1, 'micro_batch_size': 1, 'gradient_accumulation_steps': 4, 'max_iters': 10000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 10000, 'min_lr': 6e-06}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model with {'name': 'phi-1_5', 'hf_config': {'org': 'microsoft', 'name': 'phi-1_5'}, 'block_size': 2048, 'vocab_size': 50257, 'padding_multiple': 512, 'padded_vocab_size': 51200, 'n_layer': 24, 'n_head': 32, 'n_embd': 2048, 'rotary_percentage': 0.5, 'parallel_residual': True, 'bias': True, 'lm_head_bias': True, 'n_query_groups': 32, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'tanh', 'intermediate_size': 8192, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 32}\n",
            "Time to instantiate model: 0.48 seconds.\n",
            "Total parameters 1,418,270,720\n",
            "Estimated TFLOPs: 21.55\n",
            "Measured TFLOPs: 18.61\n",
            "iter 0 step 0: loss 11.2392, LR: 0.000000, iter time: 1185.55ms\n",
            "iter 100 step 25: loss 7.3394, LR: 0.000300, iter time: 98.27ms\n",
            "iter 200 step 50: loss 7.5169, LR: 0.000600, iter time: 98.50ms\n",
            "iter 300 step 75: loss 6.0452, LR: 0.000900, iter time: 98.71ms\n",
            "iter 400 step 100: loss 6.5519, LR: 0.001200, iter time: 98.37ms\n",
            "iter 500 step 125: loss 6.7917, LR: 0.001500, iter time: 98.28ms\n",
            "iter 600 step 150: loss 6.6391, LR: 0.001800, iter time: 98.33ms\n",
            "iter 700 step 175: loss 6.4404, LR: 0.002100, iter time: 98.46ms\n",
            "iter 800 step 200: loss 6.5202, LR: 0.002400, iter time: 98.37ms\n",
            "iter 900 step 225: loss 5.9671, LR: 0.002700, iter time: 98.34ms\n",
            "iter 1000 step 250: loss 6.3471, LR: 0.003000, iter time: 98.36ms\n",
            "iter 1100 step 275: loss 6.1164, LR: 0.003300, iter time: 98.75ms\n",
            "iter 1200 step 300: loss 6.3266, LR: 0.003600, iter time: 98.35ms\n",
            "iter 1300 step 325: loss 5.9665, LR: 0.003900, iter time: 98.55ms\n",
            "iter 1400 step 350: loss 6.2572, LR: 0.004200, iter time: 98.58ms\n",
            "iter 1500 step 375: loss 5.6074, LR: 0.004500, iter time: 98.65ms\n",
            "iter 1600 step 400: loss 6.1704, LR: 0.004800, iter time: 98.46ms\n",
            "iter 1700 step 425: loss 5.4533, LR: 0.005100, iter time: 98.46ms\n",
            "iter 1800 step 450: loss 6.1884, LR: 0.005400, iter time: 98.21ms\n",
            "iter 1900 step 475: loss 5.4430, LR: 0.005700, iter time: 98.33ms\n",
            "iter 2000 step 500: loss 5.7043, LR: 0.006000, iter time: 98.38ms\n",
            "iter 2100 step 525: loss 5.0510, LR: 0.005998, iter time: 98.31ms\n",
            "iter 2200 step 550: loss 6.0124, LR: 0.005991, iter time: 98.36ms\n",
            "iter 2300 step 575: loss 5.9044, LR: 0.005979, iter time: 98.45ms\n",
            "iter 2400 step 600: loss 6.0917, LR: 0.005963, iter time: 98.37ms\n",
            "iter 2500 step 625: loss 6.6045, LR: 0.005942, iter time: 98.70ms\n",
            "iter 2600 step 650: loss 6.0853, LR: 0.005917, iter time: 98.70ms\n",
            "iter 2700 step 675: loss 6.0182, LR: 0.005887, iter time: 98.55ms\n",
            "iter 2800 step 700: loss 5.5636, LR: 0.005853, iter time: 98.51ms\n",
            "iter 2900 step 725: loss 5.9271, LR: 0.005815, iter time: 98.35ms\n",
            "iter 3000 step 750: loss 6.3014, LR: 0.005772, iter time: 98.37ms\n",
            "iter 3100 step 775: loss 6.2879, LR: 0.005725, iter time: 98.31ms\n",
            "iter 3200 step 800: loss 6.1794, LR: 0.005673, iter time: 98.28ms\n",
            "iter 3300 step 825: loss 6.1148, LR: 0.005618, iter time: 98.57ms\n",
            "iter 3400 step 850: loss 5.6695, LR: 0.005558, iter time: 98.34ms\n",
            "iter 3500 step 875: loss 6.0711, LR: 0.005495, iter time: 98.60ms\n",
            "iter 3600 step 900: loss 6.2474, LR: 0.005428, iter time: 98.52ms\n",
            "iter 3700 step 925: loss 6.6012, LR: 0.005357, iter time: 98.55ms\n",
            "iter 3800 step 950: loss 5.4183, LR: 0.005282, iter time: 98.35ms\n",
            "iter 3900 step 975: loss 6.2819, LR: 0.005204, iter time: 98.60ms\n",
            "Saving checkpoint to 'out/phi-1_5/iter-003999-ckpt.pth'\n",
            "iter 4000 step 1000: loss 6.2432, LR: 0.005122, iter time: 123.81ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-62e995e9e506>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m setup(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_data_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/sample100MB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-ffa57fe1adf1>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(devices, train_data_dir, val_data_dir, precision, resume)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfabric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFabric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloggers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    837\u001b[0m                 \u001b[0;34m\" with a function that contains the code to launch in processes.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             )\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_and_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\u001b[0m in \u001b[0;36m_wrap_and_launch\u001b[0;34m(self, to_run, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_with_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_run\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\u001b[0m in \u001b[0;36m_wrap_with_setup\u001b[0;34m(self, to_run, *args, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_replace_dunder_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_replace_dunder_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchSampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mto_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e91de6fa61b3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(fabric, train_data_dir, val_data_dir, resume)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfabric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed_monitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training time: {(time.perf_counter()-train_time):.2f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-491d1e666a06>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(fabric, state, train_dataloader, val_dataloader, speed_monitor)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunked_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accumulating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, tensor, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deepspeed_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     def clip_gradients(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/strategies/strategy.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, tensor, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;34mr\"\"\"Forwards backward-calls to the precision plugin.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/plugins/precision/amp.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, tensor, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     def optimizer_step(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/plugins/precision/precision.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, tensor, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#torch.set_float32_matmul_precision(\"medium\")\n",
        "torch.cuda.amp.autocast(enabled=True)\n",
        "\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(\"data/sample100MB\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M6ZeZF9pP_RG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}